{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta 4/4/2022 TensorFlow2 Introducing the Keras Functional API (not on Vertex AI Platform)\n",
    "# src course Introduction to TensorFlow\n",
    "# git clone https://github.com/GoogleCloudPlatform/training-data-analyst \n",
    "# file src: https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/labs/3_keras_functional_api.ipynb\n",
    "\n",
    "#infra: work laptop, env anya_tf2\n",
    "#$issue with plotting model - figure out graphviz on Windows, for now - draw a static image\n",
    "#  tried\n",
    "#  pip install pydotplus, pip install graphviz\n",
    "#  pydot 1.4.2 (already installed), pydotplus 2.0.2, graphviz 0.19.1\n",
    "#  added to env vars: C:\\Anaconda3\\Lib\\site-packages and C:\\Anaconda3\\Lib\\site-packages\\graphviz\n",
    "#  also tried \n",
    "#  conda install graphviz but \n",
    "#  get security error CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://repo.anaconda.com/pkgs/main/win-64/current_repodata.json>\n",
    "#  ...current network has https://www.anaconda.com blocked\n",
    "\n",
    "#In the notebook interface, navigate to \n",
    "#  training-data-analyst > courses > machine_learning > deepdive2 > introduction_to_tensorflow > labs, \n",
    "#  and open 4_keras_functional_api.ipynb\n",
    "#Look at the complete solution, navigate to \n",
    "#  training-data-analyst > courses > machine_learning > deepdive2 > introduction_to_tensorflow > solutions, \n",
    "#  and open 4_keras_functional_api.ipynb\n",
    "\n",
    "#history\n",
    "#4/4/2022 REVIEW\n",
    "#      \n",
    "#      Understand embeddings and how to create them with the feature column API.\n",
    "#      Understand Deep and Wide models and when to use them.\n",
    "#      Understand the Keras functional API and how to build a deep and wide model with it.\n",
    "#      High-level model evaluation\n",
    "#      Make predictions with the Keras model\n",
    "#\n",
    "#      here: Build a Wide & Deep model using the Keras Functional API; call our model for online prediciton.\n",
    "#      $note: Deploy and predict in GCP and Vertex AI wouldn't work outside of GCP, refer to solution file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Keras Functional API on Vertex AI Platform\n",
    "\n",
    "**Learning objectives**\n",
    "  1. Understand embeddings and how to create them with the feature column API.\n",
    "  1. Understand Deep and Wide models and when to use them.\n",
    "  1. Understand the Keras functional API and how to build a deep and wide model with it.\n",
    "  1. Learn how to deploy the Model to Vertex AI and make predictions with the Keras model.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the last notebook, you learned about the Keras Sequential API. The [Keras Functional API](https://www.tensorflow.org/guide/keras#functional_api) provides an alternate way of building models which is more flexible. With the Functional API, we can build models with more complex topologies, multiple input or output layers, shared layers or non-sequential data flows (e.g. residual layers).\n",
    "\n",
    "In this notebook you'll use what we learned about feature columns to build a Wide & Deep model. Recall, that the idea behind Wide & Deep models is to join the two methods of learning through memorization and generalization by making a wide linear model and a deep learning model to accommodate both. You can have a look at the original research paper here: [Wide & Deep Learning for Recommender Systems](https://arxiv.org/abs/1606.07792).\n",
    "\n",
    "<img src='images/4_keras_funcAPI/wide_deep.png' width='80%'>\n",
    "<sup>(image: https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html)</sup>\n",
    "\n",
    "The Wide part of the model is associated with the memory element. In this case, we train a linear model with a wide set of crossed features and learn the correlation of this related data with the assigned label. The Deep part of the model is associated with the generalization element where we use embedding vectors for features. The best embeddings are then learned through the training process. While both of these methods can work well alone, Wide & Deep models excel by combining these techniques together.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__  in this student lab notebook -- try to complete this notebook first and then review the [solution notebook](../solutions/4_keras_functional_api.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chown command to change the ownership of the repository.\n",
    "#$my !sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the right version of Tensorflow is installed.\n",
    "#$my !pip freeze | grep tensorflow==2.3.0 || pip install tensorflow==2.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kindly ignore the deprecation warnings and incompatibility errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required numpy version.\n",
    "#$my !pip install numpy==1.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kindly ignore the deprecation warnings and incompatibility errors.\n",
    "\n",
    "**Restart** the kernel before proceeding further (On the Notebook menu, select Kernel > Restart Kernel > Restart).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the necessary libraries for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.3\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules/libraries such as numpy, pandas and datetime.\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import feature_column as fc\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Input, Dense, DenseFeatures, concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "print(tf.__version__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data \n",
    "\n",
    "We will use the taxifare dataset, using the CSV files that we created in the first notebook of this sequence. Those files have been saved into `../data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$my - works in Linux/Unix system, ie. G Colab\n",
    "#!ls -l ../data/*.csv\n",
    "\n",
    "#!head ../data/taxi*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 chq-anyac 1049089 123590 Mar 23 13:11 data/taxi_data/taxi-test.csv\n",
      "-rw-r--r-- 1 chq-anyac 1049089 579055 Mar 23 13:11 data/taxi_data/taxi-train.csv\n",
      "-rw-r--r-- 1 chq-anyac 1049089 123114 Mar 23 13:11 data/taxi_data/taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -l data/taxi_data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3,2011-01-28 20:42:59 UTC,-73.999022,40.739146,-73.990369,40.717866,1,0\n",
      "7.7,2011-06-27 04:28:06 UTC,-73.987443,40.729221,-73.979013,40.758641,1,1\n",
      "10.5,2011-04-03 00:54:53 UTC,-73.982539,40.735725,-73.954797,40.778388,1,2\n",
      "16.2,2009-04-10 04:11:56 UTC,-74.001945,40.740505,-73.91385,40.758559,1,3\n",
      "33.5,2014-02-24 18:22:00 UTC,-73.993372,40.753382,-73.8609,40.732897,2,4\n",
      "6.9,2011-12-10 00:25:23 UTC,-73.996237,40.721848,-73.989416,40.718052,1,5\n",
      "6.1,2012-09-01 14:30:19 UTC,-73.977048,40.758461,-73.984899,40.744693,2,6\n",
      "9.5,2012-11-08 13:28:07 UTC,-73.969402,40.757545,-73.950049,40.776079,1,7\n",
      "9.0,2014-07-15 11:37:25 UTC,-73.979318,40.760949,-73.95767,40.773724,1,8\n",
      "3.3,2009-11-09 18:06:58 UTC,-73.955675,40.779154,-73.961172,40.772368,1,9\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head data/taxi_data/taxi-train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tf.data to read the CSV files\n",
    "\n",
    "We wrote these functions for reading data from the csv files above in the [previous notebook](2_dataset_api.ipynb). For this lab we will also include some additional engineered features in our model. In particular, we will compute the difference in latitude and longitude, as well as the Euclidean distance between the pick-up and drop-off locations. We can accomplish this by adding these new features to the features dictionary with the function `add_engineered_features` below. \n",
    "\n",
    "Note that we include a call to this function when collecting our features dict and labels in the `features_and_labels` function below as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = [\n",
    "    'fare_amount',\n",
    "    'pickup_datetime',\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count',\n",
    "    'key'\n",
    "]\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]\n",
    "UNWANTED_COLS = ['pickup_datetime', 'key']\n",
    "\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    features = row_data\n",
    "        \n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        features.pop(unwanted_col)\n",
    "\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def create_dataset(pattern, batch_size=1, mode='eval'):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "\n",
    "    dataset = dataset.map(features_and_labels)\n",
    "    \n",
    "    if mode == 'train':\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "\n",
    "    # take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature columns for Wide and Deep model\n",
    "\n",
    "For the Wide columns, we will create feature columns of crossed features. To do this, we'll create a collection of Tensorflow feature columns to pass to the `tf.feature_column.crossed_column` constructor. The Deep columns will consist of numeric columns and the embedding columns we want to create. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #1:** In the cell below, create feature columns for our wide-and-deep model. You'll need to build \n",
    "1. bucketized columns using [tf.feature_column.bucketized_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column) for the pickup and dropoff latitude and longitude,\n",
    "2. crossed columns using [tf.feature_column.crossed_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/crossed_column) for those bucketized columns, and \n",
    "3. embedding columns using [tf.feature_column.embedding_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column) for the crossed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38.0, 38.266666666666666, 38.53333333333333, 38.8, 39.06666666666667, 39.333333333333336, 39.6, 39.86666666666667, 40.13333333333333, 40.4, 40.666666666666664, 40.93333333333333, 41.2, 41.46666666666667, 41.733333333333334, 42.0]\n",
      "\n",
      "[-76.0, -75.73333333333333, -75.46666666666667, -75.2, -74.93333333333334, -74.66666666666667, -74.4, -74.13333333333334, -73.86666666666666, -73.6, -73.33333333333333, -73.06666666666666, -72.8, -72.53333333333333, -72.26666666666667, -72.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BucketizedColumn(source_column=NumericColumn(key='pickup_longitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-76.0, -75.73333333333333, -75.46666666666667, -75.2, -74.93333333333334, -74.66666666666667, -74.4, -74.13333333333334, -73.86666666666666, -73.6, -73.33333333333333, -73.06666666666666, -72.8, -72.53333333333333, -72.26666666666667, -72.0))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 1\n",
    "\n",
    "# 1. Bucketize latitudes and longitudes\n",
    "NBUCKETS = 16\n",
    "latbuckets = np.linspace(start=38.0, stop=42.0, num=NBUCKETS).tolist()\n",
    "lonbuckets = np.linspace(start=-76.0, stop=-72.0, num=NBUCKETS).tolist()\n",
    "\n",
    "# TODO: Your code goes here.\n",
    "fc_bucketized_plon = fc.bucketized_column(\n",
    "    source_column=fc.numeric_column(\"pickup_longitude\"), boundaries=lonbuckets)\n",
    "fc_bucketized_plat = fc.bucketized_column(\n",
    "    source_column=fc.numeric_column(\"pickup_latitude\"), boundaries=latbuckets)\n",
    "fc_bucketized_dlon = fc.bucketized_column(\n",
    "    source_column=fc.numeric_column(\"dropoff_longitude\"), boundaries=lonbuckets)\n",
    "fc_bucketized_dlat = fc.bucketized_column(\n",
    "    source_column=fc.numeric_column(\"dropoff_latitude\"), boundaries=latbuckets)\n",
    "\n",
    "#$my preview\n",
    "print(latbuckets)\n",
    "print()\n",
    "print(lonbuckets)\n",
    "\n",
    "fc_bucketized_plon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossedColumn(keys=(BucketizedColumn(source_column=NumericColumn(key='dropoff_longitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-76.0, -75.73333333333333, -75.46666666666667, -75.2, -74.93333333333334, -74.66666666666667, -74.4, -74.13333333333334, -73.86666666666666, -73.6, -73.33333333333333, -73.06666666666666, -72.8, -72.53333333333333, -72.26666666666667, -72.0)), BucketizedColumn(source_column=NumericColumn(key='dropoff_latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(38.0, 38.266666666666666, 38.53333333333333, 38.8, 39.06666666666667, 39.333333333333336, 39.6, 39.86666666666667, 40.13333333333333, 40.4, 40.666666666666664, 40.93333333333333, 41.2, 41.46666666666667, 41.733333333333334, 42.0))), hash_bucket_size=100, hash_key=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_crossed_dloc = fc.crossed_column([fc_bucketized_dlon, fc_bucketized_dlat], hash_bucket_size=100)\n",
    "fc_crossed_dloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossedColumn(keys=(BucketizedColumn(source_column=NumericColumn(key='dropoff_latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(38.0, 38.266666666666666, 38.53333333333333, 38.8, 39.06666666666667, 39.333333333333336, 39.6, 39.86666666666667, 40.13333333333333, 40.4, 40.666666666666664, 40.93333333333333, 41.2, 41.46666666666667, 41.733333333333334, 42.0)), BucketizedColumn(source_column=NumericColumn(key='dropoff_longitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-76.0, -75.73333333333333, -75.46666666666667, -75.2, -74.93333333333334, -74.66666666666667, -74.4, -74.13333333333334, -73.86666666666666, -73.6, -73.33333333333333, -73.06666666666666, -72.8, -72.53333333333333, -72.26666666666667, -72.0))), hash_bucket_size=256, hash_key=None)\n",
      "\n",
      "CrossedColumn(keys=(CrossedColumn(keys=(BucketizedColumn(source_column=NumericColumn(key='dropoff_latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(38.0, 38.266666666666666, 38.53333333333333, 38.8, 39.06666666666667, 39.333333333333336, 39.6, 39.86666666666667, 40.13333333333333, 40.4, 40.666666666666664, 40.93333333333333, 41.2, 41.46666666666667, 41.733333333333334, 42.0)), BucketizedColumn(source_column=NumericColumn(key='dropoff_longitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-76.0, -75.73333333333333, -75.46666666666667, -75.2, -74.93333333333334, -74.66666666666667, -74.4, -74.13333333333334, -73.86666666666666, -73.6, -73.33333333333333, -73.06666666666666, -72.8, -72.53333333333333, -72.26666666666667, -72.0))), hash_bucket_size=256, hash_key=None), CrossedColumn(keys=(BucketizedColumn(source_column=NumericColumn(key='pickup_latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(38.0, 38.266666666666666, 38.53333333333333, 38.8, 39.06666666666667, 39.333333333333336, 39.6, 39.86666666666667, 40.13333333333333, 40.4, 40.666666666666664, 40.93333333333333, 41.2, 41.46666666666667, 41.733333333333334, 42.0)), BucketizedColumn(source_column=NumericColumn(key='pickup_longitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-76.0, -75.73333333333333, -75.46666666666667, -75.2, -74.93333333333334, -74.66666666666667, -74.4, -74.13333333333334, -73.86666666666666, -73.6, -73.33333333333333, -73.06666666666666, -72.8, -72.53333333333333, -72.26666666666667, -72.0))), hash_bucket_size=256, hash_key=None)), hash_bucket_size=65536, hash_key=None)\n"
     ]
    }
   ],
   "source": [
    "# 2. Cross features for locations\n",
    "# TODO: Your code goes here.\n",
    "fc_crossed_dloc = fc.crossed_column(\n",
    "    [fc_bucketized_dlat, fc_bucketized_dlon],\n",
    "    hash_bucket_size=NBUCKETS * NBUCKETS)\n",
    "fc_crossed_ploc = fc.crossed_column(\n",
    "    [fc_bucketized_plat, fc_bucketized_plon],\n",
    "    hash_bucket_size=NBUCKETS * NBUCKETS)\n",
    "fc_crossed_pd_pair = fc.crossed_column(\n",
    "    [fc_crossed_dloc, fc_crossed_ploc],\n",
    "    hash_bucket_size=NBUCKETS**4)\n",
    "\n",
    "#$my preview\n",
    "print(fc_crossed_dloc)\n",
    "print()\n",
    "print(fc_crossed_pd_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingColumn(categorical_column=CrossedColumn(keys=(BucketizedColumn(source_column=NumericColumn(key='dropoff_latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(38.0, 38.266666666666666, 38.53333333333333, 38.8, 39.06666666666667, 39.333333333333336, 39.6, 39.86666666666667, 40.13333333333333, 40.4, 40.666666666666664, 40.93333333333333, 41.2, 41.46666666666667, 41.733333333333334, 42.0)), BucketizedColumn(source_column=NumericColumn(key='dropoff_longitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-76.0, -75.73333333333333, -75.46666666666667, -75.2, -74.93333333333334, -74.66666666666667, -74.4, -74.13333333333334, -73.86666666666666, -73.6, -73.33333333333333, -73.06666666666666, -72.8, -72.53333333333333, -72.26666666666667, -72.0))), hash_bucket_size=256, hash_key=None), dimension=3, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x000001DF213C36C8>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True)\n"
     ]
    }
   ],
   "source": [
    "# 3. Create embedding columns for the crossed columns\n",
    "# TODO: Your code goes here.\n",
    "fc_pd_pair = fc.embedding_column(categorical_column=fc_crossed_pd_pair, dimension=3)\n",
    "fc_dloc = fc.embedding_column(categorical_column=fc_crossed_dloc, dimension=3)\n",
    "fc_ploc = fc.embedding_column(categorical_column=fc_crossed_ploc, dimension=3)\n",
    "\n",
    "#$my preview\n",
    "print(fc_dloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather list of feature columns\n",
    "\n",
    "Next we gather the list of wide and deep feature columns we'll pass to our Wide & Deep model in Tensorflow. Recall, wide columns are sparse, have linear relationship with the output while continuous columns are deep, have a complex relationship with the output. We will use our previously bucketized columns to collect crossed feature columns and sparse feature columns for our wide columns, and embedding feature columns and numeric features columns for the deep columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #2:** Collect the wide and deep columns into two separate lists. You'll have two lists: One called `wide_columns` containing the one-hot encoded features from the crossed features and one called `deep_columns` which contains numeric and embedding feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2\n",
    "wide_columns = [\n",
    "    # One-hot encoded feature crosses\n",
    "    # TODO: Your code goes here.\n",
    "    fc.indicator_column(fc_crossed_dloc),\n",
    "    fc.indicator_column(fc_crossed_ploc),\n",
    "    fc.indicator_column(fc_crossed_pd_pair)\n",
    "]\n",
    "\n",
    "deep_columns = [\n",
    "    # Embedding_column to \"group\" together ...\n",
    "    # TODO: Your code goes here.\n",
    "    fc.embedding_column(fc_crossed_pd_pair, dimension=10),\n",
    "\n",
    "    # Numeric columns\n",
    "    # TODO: Your code goes here.\n",
    "    fc.numeric_column(\"pickup_latitude\"),\n",
    "    fc.numeric_column(\"pickup_longitude\"),\n",
    "    fc.numeric_column(\"dropoff_longitude\"),\n",
    "    fc.numeric_column(\"dropoff_latitude\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndicatorColumn(categorical_column=CrossedColumn(keys=(BucketizedColumn(source_column=NumericColumn(key='dropoff_latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(38.0, 38.266666666666666, 38.53333333333333, 38.8, 39.06666666666667, 39.333333333333336, 39.6, 39.86666666666667, 40.13333333333333, 40.4, 40.666666666666664, 40.93333333333333, 41.2, 41.46666666666667, 41.733333333333334, 42.0)), BucketizedColumn(source_column=NumericColumn(key='dropoff_longitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-76.0, -75.73333333333333, -75.46666666666667, -75.2, -74.93333333333334, -74.66666666666667, -74.4, -74.13333333333334, -73.86666666666666, -73.6, -73.33333333333333, -73.06666666666666, -72.8, -72.53333333333333, -72.26666666666667, -72.0))), hash_bucket_size=256, hash_key=None))\n",
      "\n",
      "EmbeddingColumn(categorical_column=CrossedColumn(keys=(CrossedColumn(keys=(BucketizedColumn(source_column=NumericColumn(key='dropoff_latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(38.0, 38.266666666666666, 38.53333333333333, 38.8, 39.06666666666667, 39.333333333333336, 39.6, 39.86666666666667, 40.13333333333333, 40.4, 40.666666666666664, 40.93333333333333, 41.2, 41.46666666666667, 41.733333333333334, 42.0)), BucketizedColumn(source_column=NumericColumn(key='dropoff_longitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-76.0, -75.73333333333333, -75.46666666666667, -75.2, -74.93333333333334, -74.66666666666667, -74.4, -74.13333333333334, -73.86666666666666, -73.6, -73.33333333333333, -73.06666666666666, -72.8, -72.53333333333333, -72.26666666666667, -72.0))), hash_bucket_size=256, hash_key=None), CrossedColumn(keys=(BucketizedColumn(source_column=NumericColumn(key='pickup_latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(38.0, 38.266666666666666, 38.53333333333333, 38.8, 39.06666666666667, 39.333333333333336, 39.6, 39.86666666666667, 40.13333333333333, 40.4, 40.666666666666664, 40.93333333333333, 41.2, 41.46666666666667, 41.733333333333334, 42.0)), BucketizedColumn(source_column=NumericColumn(key='pickup_longitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-76.0, -75.73333333333333, -75.46666666666667, -75.2, -74.93333333333334, -74.66666666666667, -74.4, -74.13333333333334, -73.86666666666666, -73.6, -73.33333333333333, -73.06666666666666, -72.8, -72.53333333333333, -72.26666666666667, -72.0))), hash_bucket_size=256, hash_key=None)), hash_bucket_size=65536, hash_key=None), dimension=10, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x000001DF213B3188>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True)\n"
     ]
    }
   ],
   "source": [
    "#$my preview\n",
    "print(wide_columns[0])\n",
    "print()\n",
    "print(deep_columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Wide and Deep model in Keras\n",
    "\n",
    "To build a wide-and-deep network, we connect the sparse (i.e. wide) features directly to the output node, but pass the dense (i.e. deep) features through a set of fully connected layers. Hereâ€™s that model architecture looks using the Functional API.\n",
    "\n",
    "First, we'll create our input columns using [tf.keras.layers.Input](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLS = [\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count'\n",
    "]\n",
    "\n",
    "inputs = {colname : Input(name=colname, shape=(), dtype='float32')\n",
    "          for colname in INPUT_COLS\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our custom RMSE evaluation metric and build our wide and deep model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #3:** Complete the code in the function `build_model` below so that it returns a compiled Keras model. The argument `dnn_hidden_units` should represent the number of units in each layer of your network. Use the Functional API to build a wide-and-deep model. Use the `deep_columns` you created above to build the deep layers and the `wide_columns` to create the wide layers. Once you have the wide and deep components, you will combine them to feed to a final fully connected layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "# TODO 3\n",
    "def build_model(dnn_hidden_units):\n",
    "    # Create the deep part of model\n",
    "    # TODO: Your code goes here.\n",
    "    deep = DenseFeatures(deep_columns, name='deep_inputs')(inputs)\n",
    "    for num_nodes in dnn_hidden_units:\n",
    "        deep = Dense(num_nodes, activation='relu')(deep) \n",
    "    \n",
    "    # Create the wide part of model.\n",
    "    wide = DenseFeatures(wide_columns, name='wide_inputs')(inputs) # TODO: Your code goes here\n",
    "\n",
    "    # Combine deep and wide parts of the model\n",
    "    combined = concatenate(inputs=[deep, wide], name='combined') # TODO: Your code goes here.\n",
    "\n",
    "    # Map the combined outputs into a single prediction value\n",
    "    output = Dense(units=1, activation=None, name='prediction')(combined) # TODO: Your code goes here.\n",
    "    \n",
    "    # Finalize the model\n",
    "    model = Model(inputs=list(inputs.values()), outputs=output) # TODO: Your code goes here.\n",
    "\n",
    "    # Compile the keras model\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"]) # TODO: Your code goes here.\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can call the `build_model` to create the model. Here we'll have two hidden layers, each with 10 neurons, for the deep part of our model. We can also use `plot_model` to see a diagram of the model we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_UNITS = [10,10]\n",
    "\n",
    "model = build_model(dnn_hidden_units=HIDDEN_UNITS)\n",
    "model.summary() #$my view model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "#$my $actodo figure out graphviz on Windows, for now - draw a static image\n",
    "tf.keras.utils.plot_model(model, show_shapes=False, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model](images\\4_keras_funcAPI\\plot_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up our training variables, create our datasets for training and validation, and train our model.\n",
    "\n",
    "(We refer you the the blog post [ML Design Pattern #3: Virtual Epochs](https://medium.com/google-cloud/ml-design-pattern-3-virtual-epochs-f842296de730) for further details on why express the training in terms of `NUM_TRAIN_EXAMPLES` and `NUM_EVALS` and why, in this training code, the number of epochs is really equal to the number of evaluations we perform.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 5  # training dataset will repeat, wrap around\n",
    "NUM_EVALS = 50  # how many times to evaluate\n",
    "NUM_EVAL_EXAMPLES = 10000  # enough to get a reasonable sample\n",
    "\n",
    "trainds = create_dataset(\n",
    "    pattern='data/taxi_data/taxi-train*', #$my\n",
    "    batch_size=BATCH_SIZE,\n",
    "    mode='train')\n",
    "\n",
    "evalds = create_dataset(\n",
    "    pattern='data/taxi_data/taxi-valid*', #$my\n",
    "    batch_size=BATCH_SIZE,\n",
    "    mode='eval').take(NUM_EVAL_EXAMPLES//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 22s 22s/step - loss: 235.4780 - rmse: 15.3453 - mse: 235.4780 - val_loss: 248.1456 - val_rmse: 15.7426 - val_mse: 248.1456\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 211.0613 - rmse: 14.5280 - mse: 211.0613 - val_loss: 247.2041 - val_rmse: 15.7172 - val_mse: 247.2041\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 211.4760 - rmse: 14.5422 - mse: 211.4760 - val_loss: 243.1478 - val_rmse: 15.5872 - val_mse: 243.1478\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 231.3312 - rmse: 15.2096 - mse: 231.3312 - val_loss: 240.4752 - val_rmse: 15.4963 - val_mse: 240.4752\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 215.1457 - rmse: 14.6678 - mse: 215.1457 - val_loss: 240.7142 - val_rmse: 15.5082 - val_mse: 240.7142\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 228.6791 - rmse: 15.1221 - mse: 228.6791 - val_loss: 237.8982 - val_rmse: 15.4113 - val_mse: 237.8982\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 206.2918 - rmse: 14.3629 - mse: 206.2918 - val_loss: 237.7245 - val_rmse: 15.4072 - val_mse: 237.7245\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 200.2583 - rmse: 14.1513 - mse: 200.2583 - val_loss: 234.8453 - val_rmse: 15.3020 - val_mse: 234.8453\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 241.0809 - rmse: 15.5268 - mse: 241.0809 - val_loss: 232.9421 - val_rmse: 15.2594 - val_mse: 232.9421\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 203.1478 - rmse: 14.2530 - mse: 203.1478 - val_loss: 228.5157 - val_rmse: 15.1044 - val_mse: 228.5157\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 199.1907 - rmse: 14.1135 - mse: 199.1907 - val_loss: 227.8732 - val_rmse: 15.0874 - val_mse: 227.8732\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 198.1836 - rmse: 14.0778 - mse: 198.1836 - val_loss: 225.6935 - val_rmse: 15.0134 - val_mse: 225.6935\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 173.0953 - rmse: 13.1566 - mse: 173.0953 - val_loss: 222.2126 - val_rmse: 14.9029 - val_mse: 222.2126\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 198.5813 - rmse: 14.0919 - mse: 198.5813 - val_loss: 220.2650 - val_rmse: 14.8345 - val_mse: 220.2650\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 182.0569 - rmse: 13.4928 - mse: 182.0569 - val_loss: 217.1724 - val_rmse: 14.7281 - val_mse: 217.1724\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 224.8531 - rmse: 14.9951 - mse: 224.8531 - val_loss: 215.6958 - val_rmse: 14.6799 - val_mse: 215.6958\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 160.2537 - rmse: 12.6591 - mse: 160.2537 - val_loss: 212.3163 - val_rmse: 14.5645 - val_mse: 212.3163\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 181.1113 - rmse: 13.4578 - mse: 181.1113 - val_loss: 209.1347 - val_rmse: 14.4519 - val_mse: 209.1347\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 200.7524 - rmse: 14.1687 - mse: 200.7524 - val_loss: 207.2832 - val_rmse: 14.3922 - val_mse: 207.2832\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 189.7434 - rmse: 13.7747 - mse: 189.7434 - val_loss: 203.7604 - val_rmse: 14.2615 - val_mse: 203.7604\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 179.6882 - rmse: 13.4048 - mse: 179.6882 - val_loss: 202.9176 - val_rmse: 14.2256 - val_mse: 202.9176\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 167.3728 - rmse: 12.9373 - mse: 167.3728 - val_loss: 199.6848 - val_rmse: 14.1278 - val_mse: 199.6848\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 208.3966 - rmse: 14.4359 - mse: 208.3966 - val_loss: 196.3103 - val_rmse: 14.0014 - val_mse: 196.3103\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 189.4422 - rmse: 13.7638 - mse: 189.4422 - val_loss: 197.4966 - val_rmse: 14.0509 - val_mse: 197.4966\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 170.6616 - rmse: 13.0638 - mse: 170.6616 - val_loss: 193.8413 - val_rmse: 13.9126 - val_mse: 193.8413\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 168.1503 - rmse: 12.9673 - mse: 168.1503 - val_loss: 190.1175 - val_rmse: 13.7805 - val_mse: 190.1175\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 154.3194 - rmse: 12.4225 - mse: 154.3194 - val_loss: 187.0273 - val_rmse: 13.6660 - val_mse: 187.0273\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 175.4844 - rmse: 13.2471 - mse: 175.4844 - val_loss: 182.6737 - val_rmse: 13.4978 - val_mse: 182.6737\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 148.3845 - rmse: 12.1813 - mse: 148.3845 - val_loss: 181.9669 - val_rmse: 13.4819 - val_mse: 181.9669\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 126.8524 - rmse: 11.2629 - mse: 126.8524 - val_loss: 181.2319 - val_rmse: 13.4483 - val_mse: 181.2319\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 169.7460 - rmse: 13.0287 - mse: 169.7460 - val_loss: 176.3633 - val_rmse: 13.2773 - val_mse: 176.3633\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 142.7381 - rmse: 11.9473 - mse: 142.7381 - val_loss: 174.0128 - val_rmse: 13.1694 - val_mse: 174.0128\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 174.7129 - rmse: 13.2179 - mse: 174.7129 - val_loss: 170.3606 - val_rmse: 13.0441 - val_mse: 170.3606\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 132.8822 - rmse: 11.5275 - mse: 132.8822 - val_loss: 168.5482 - val_rmse: 12.9744 - val_mse: 168.5482\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 189.2820 - rmse: 13.7580 - mse: 189.2820 - val_loss: 166.9283 - val_rmse: 12.9115 - val_mse: 166.9283\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 145.2115 - rmse: 12.0504 - mse: 145.2115 - val_loss: 161.0921 - val_rmse: 12.6693 - val_mse: 161.0921\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 6s 6s/step - loss: 128.3203 - rmse: 11.3279 - mse: 128.3203 - val_loss: 160.3427 - val_rmse: 12.6573 - val_mse: 160.3427\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 159.4450 - rmse: 12.6272 - mse: 159.4450 - val_loss: 157.3777 - val_rmse: 12.5277 - val_mse: 157.3777\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 149.2581 - rmse: 12.2171 - mse: 149.2581 - val_loss: 155.5318 - val_rmse: 12.4493 - val_mse: 155.5318\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 154.4453 - rmse: 12.4276 - mse: 154.4453 - val_loss: 151.7409 - val_rmse: 12.2982 - val_mse: 151.7409\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 113.8643 - rmse: 10.6707 - mse: 113.8643 - val_loss: 149.3961 - val_rmse: 12.2017 - val_mse: 149.3961\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 113.5573 - rmse: 10.6563 - mse: 113.5573 - val_loss: 145.0502 - val_rmse: 12.0305 - val_mse: 145.0502\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 154.8068 - rmse: 12.4421 - mse: 154.8068 - val_loss: 142.6202 - val_rmse: 11.9321 - val_mse: 142.6202\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 104.9633 - rmse: 10.2452 - mse: 104.9633 - val_loss: 140.1727 - val_rmse: 11.8320 - val_mse: 140.1727\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 110.3870 - rmse: 10.5065 - mse: 110.3870 - val_loss: 139.2143 - val_rmse: 11.7898 - val_mse: 139.2143\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 110.1141 - rmse: 10.4935 - mse: 110.1141 - val_loss: 137.5078 - val_rmse: 11.7050 - val_mse: 137.5078\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 4s 4s/step - loss: 113.7759 - rmse: 10.6666 - mse: 113.7759 - val_loss: 134.5341 - val_rmse: 11.5853 - val_mse: 134.5341\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step - loss: 87.7611 - rmse: 9.3681 - mse: 87.7611 - val_loss: 131.8369 - val_rmse: 11.4620 - val_mse: 131.8369\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 132.5037 - rmse: 11.5110 - mse: 132.5037 - val_loss: 129.5656 - val_rmse: 11.3558 - val_mse: 129.5656\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 104.0319 - rmse: 10.1996 - mse: 104.0319 - val_loss: 125.7248 - val_rmse: 11.1965 - val_mse: 125.7248\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "OUTDIR = \"./models/taxi_trained\"\n",
    "shutil.rmtree(path=OUTDIR, ignore_errors=True) # start fresh each time\n",
    "\n",
    "history = model.fit(x=trainds,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=NUM_EVALS,\n",
    "                    validation_data=evalds,\n",
    "                    callbacks=[TensorBoard(OUTDIR)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, we can examine the history to see how the RMSE changes through training on the train set and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1df21f8cb08>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "RMSE_COLS = ['rmse', 'val_rmse']\n",
    "\n",
    "pd.DataFrame(history.history)[RMSE_COLS].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions with our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions with our trained model, we can call the [predict method](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict), passing to it a dictionary of values. The `steps` parameter determines the total number of steps before declaring the prediction round finished. Here since we have just one example, we set `steps=1` (setting `steps=None` would also work). Note, however, that if x is a `tf.data` dataset or a dataset iterator, and steps is set to None, predict will run until the input dataset is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.563088]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\n",
    "    x={\n",
    "        \"pickup_longitude\": tf.convert_to_tensor([-73.982683]),\n",
    "        \"pickup_latitude\": tf.convert_to_tensor([40.742104]),\n",
    "        \"dropoff_longitude\": tf.convert_to_tensor([-73.983766]),\n",
    "        \"dropoff_latitude\": tf.convert_to_tensor([40.755174]),\n",
    "        \"passenger_count\": tf.convert_to_tensor([3.0]),\n",
    "    },\n",
    "    steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export and deploy our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, making individual predictions is not realistic, because we can't expect client code to have a model object in memory. For others to use our trained model, we'll have to export our model to a file, and expect client code to instantiate the model from that exported file. \n",
    "\n",
    "We'll export the model to a TensorFlow SavedModel format. Once we have a model in this format, we have lots of ways to \"serve\" the model, from a web application, from JavaScript, from mobile applications, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./export/savedmodel\\20220404184557\\assets\n",
      "INFO:tensorflow:Assets written to: ./export/savedmodel\\20220404184557\\assets\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"./export/savedmodel\"\n",
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "EXPORT_PATH = os.path.join(OUTPUT_DIR, TIMESTAMP)\n",
    "\n",
    "tf.saved_model.save(model, EXPORT_PATH)  # with default serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['dropoff_latitude'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1)\n",
      "      name: serving_default_dropoff_latitude:0\n",
      "  inputs['dropoff_longitude'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1)\n",
      "      name: serving_default_dropoff_longitude:0\n",
      "  inputs['passenger_count'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1)\n",
      "      name: serving_default_passenger_count:0\n",
      "  inputs['pickup_latitude'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1)\n",
      "      name: serving_default_pickup_latitude:0\n",
      "  inputs['pickup_longitude'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1)\n",
      "      name: serving_default_pickup_longitude:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['prediction'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 18:46:09.526859: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-04-04 18:46:09.527487: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "FIND: Parameter format not correct\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show \\\n",
    "    --tag_set serve \\\n",
    "    --signature_def serving_default \\\n",
    "    --dir {EXPORT_PATH}\n",
    "\n",
    "!find {EXPORT_PATH}\n",
    "os.environ['EXPORT_PATH'] = EXPORT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mystop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-78e0181c830a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmystop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#stop here - the rest wouldn't work outside of GCP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mystop' is not defined"
     ]
    }
   ],
   "source": [
    "mystop\n",
    "#stop here - the rest wouldn't work outside of GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy our model to Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will deploy our trained model to Vertex AI and see how we can make online predicitons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "REGION = \"us-central1\"\n",
    "MODEL_DISPLAYNAME = f\"taxifare_keras_functional-{TIMESTAMP}\"\n",
    "\n",
    "print(f\"MODEL_DISPLAYNAME: {MODEL_DISPLAYNAME}\")\n",
    "\n",
    "# from https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-3:latest\"\n",
    ")\n",
    "\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create GCS bucket if it doesn't exist already...\n",
    "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
    "\n",
    "if [ -n \"$exists\" ]; then\n",
    "    echo -e \"Bucket exists, let's not recreate it.\"\n",
    "else\n",
    "    echo \"Creating a new GCS bucket.\"\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "    echo \"\\nHere are your current buckets:\"\n",
    "    gsutil ls\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -R $EXPORT_PATH gs://$BUCKET/$MODEL_DISPLAYNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAYNAME,\n",
    "    artifact_uri=f\"gs://{BUCKET}/{MODEL_DISPLAYNAME}\",\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_TYPE = \"n1-standard-2\"\n",
    "\n",
    "endpoint = uploaded_model.deploy(\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/taxi_fare_keras_func_model.png' width='80%'>\n",
    "<sup>(image:Your model in Vertex AI)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = {\n",
    "    \"pickup_longitude\": -73.982683,\n",
    "    \"pickup_latitude\": 40.742104,\n",
    "    \"dropoff_longitude\": -73.983766,\n",
    "    \"dropoff_latitude\": 40.755174,\n",
    "    \"passenger_count\": 3.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.predict([instance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "When deploying a model to an endpoint for online prediction, the minimum `min-replica-count` is 1, and it is charged per node hour. So let's delete the endpoint to reduce unnecessary charges. Before we can delete the endpoint, we first undeploy all attached models... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anya_tf2",
   "language": "python",
   "name": "anya_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
