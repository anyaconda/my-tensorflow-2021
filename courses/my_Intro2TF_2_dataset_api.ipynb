{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta 3/23/2022 TensorFlow Dataset API\n",
    "# src course Introduction to TensorFlow\n",
    "# git clone https://github.com/GoogleCloudPlatform/training-data-analyst \n",
    "# file src: https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/labs/2_dataset_api.ipynb\n",
    "\n",
    "#infra: work laptop, env anya_tf2\n",
    "\n",
    "#In the notebook interface, navigate to \n",
    "#  training-data-analyst > courses > machine_learning > deepdive2 > introduction_to_tensorflow > labs, \n",
    "#  and open 2_dataset_api.ipynb\n",
    "#Look at the complete solution, navigate to \n",
    "#  training-data-analyst > courses > machine_learning > deepdive2 > introduction_to_tensorflow > solutions, \n",
    "#  and open 2_dataset_api.ipynb\n",
    "\n",
    "#history\n",
    "#3/23/2022 REVIEW\n",
    "#      How to use tf.data to read data from memory\n",
    "#      How to use tf.data in a training loop\n",
    "#      How to use tf.data to read data from disk\n",
    "#      Learn how to write production input pipelines with feature engineering (batching, shuffling, etc.)\n",
    "\n",
    "#      myPreviews:\n",
    "#      Preview what's in tempds (class tensorflow.python.data.ops.dataset_ops.PrefetchDataset) - by iteration over items in ds\n",
    "#      Preview what's in tempds - without iteration\n",
    "#      Cleaner preview offered by Solution file\n",
    "#      Transforming the features\n",
    "#      $myComment: fancy create_dataset()\n",
    "#      $myComment: fancy input pipeline create_dataset()\n",
    "\n",
    "# Reference\n",
    "#Preview a .csv file\n",
    "#  my_Intro2TF_load_diff_filedata.ipynb \n",
    "#take() method of tf.data.Dataset\n",
    "#  2_dataset_api_solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Dataset API\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Learn how to use tf.data to read data from memory\n",
    "1. Learn how to use tf.data in a training loop\n",
    "1. Learn how to use tf.data to read data from disk\n",
    "1. Learn how to write production input pipelines with feature engineering (batching, shuffling, etc.)\n",
    "\n",
    "\n",
    "In this notebook, we will start by refactoring the linear regression we implemented in the previous lab so that it takes data from a`tf.data.Dataset`, and we will learn how to implement **stochastic gradient descent** with it. In this case, the original dataset will be synthetic and read by the `tf.data` API directly  from memory.\n",
    "\n",
    "In a second part, we will learn how to load a dataset with the `tf.data` API when the dataset resides on disk.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__  in this student lab notebook -- try to complete this notebook first and then review the [solution notebook](../solutions/2_dataset_api.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the synthetic dataset of the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_POINTS = 10\n",
    "X = tf.constant(range(N_POINTS), dtype=tf.float32)\n",
    "Y = 2 * X + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#$my\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(X,Y)\n",
    "plt.title(\"Toy Dataset\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.yticks(Y)\n",
    "plt.xticks(X, rotation=60);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with implementing a function that takes as input\n",
    "\n",
    "\n",
    "- our $X$ and $Y$ vectors of synthetic data generated by the linear function $y= 2x + 10$\n",
    "- the number of passes over the dataset we want to train on (`epochs`)\n",
    "- the size of the batches the dataset (`batch_size`)\n",
    "\n",
    "and returns a `tf.data.Dataset`: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Note that the last batch may not contain the exact number of elements you specified because the dataset was exhausted.\n",
    "\n",
    "If you want batches with the exact same number of elements per batch, we will have to discard the last batch by\n",
    "setting:\n",
    "\n",
    "```python\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "```\n",
    "\n",
    "We will do that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #1:** Complete the code below to \n",
    "1. instantiate a `tf.data` dataset using [tf.data.Dataset.from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices).\n",
    "2. Set up the dataset to \n",
    "  * repeat `epochs` times,\n",
    "  * create a batch of size `batch_size`, ignoring extra elements when the batch does not divide the number of input elements evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1\n",
    "def create_dataset(X, Y, epochs, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y)) # TODO -- Your code here.\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True) # TODO -- Your code here.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function by iterating twice over our dataset in batches of 3 datapoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [0. 1. 2.] y: [10. 12. 14.]\n",
      "x: [3. 4. 5.] y: [16. 18. 20.]\n",
      "x: [6. 7. 8.] y: [22. 24. 26.]\n",
      "x: [9. 0. 1.] y: [28. 10. 12.]\n",
      "x: [2. 3. 4.] y: [14. 16. 18.]\n",
      "x: [5. 6. 7.] y: [20. 22. 24.]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 3\n",
    "EPOCH = 2\n",
    "\n",
    "dataset = create_dataset(X, Y, epochs=EPOCH, batch_size=BATCH_SIZE)\n",
    "\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    print(\"x:\", x.numpy(), \"y:\", y.numpy())\n",
    "    assert len(x) == BATCH_SIZE\n",
    "    assert len(y) == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 2.], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 12., 14.], dtype=float32)>)\n",
      "1\n",
      "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([3., 4., 5.], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([16., 18., 20.], dtype=float32)>)\n",
      "2\n",
      "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([6., 7., 8.], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([22., 24., 26.], dtype=float32)>)\n",
      "3\n",
      "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([9., 0., 1.], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([28., 10., 12.], dtype=float32)>)\n",
      "4\n",
      "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([2., 3., 4.], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([14., 16., 18.], dtype=float32)>)\n",
      "5\n",
      "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5., 6., 7.], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([20., 22., 24.], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "#$my\n",
    "for i in enumerate(dataset): #class 'tuple' of len(2)\n",
    "    print(i[0])\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function and the function that computes the gradients are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(X, Y, w0, w1):\n",
    "    Y_hat = w0 * X + w1\n",
    "    errors = (Y_hat - Y)**2\n",
    "    return tf.reduce_mean(errors)\n",
    "\n",
    "\n",
    "def compute_gradients(X, Y, w0, w1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_mse(X, Y, w0, w1)\n",
    "    return tape.gradient(loss, [w0, w1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference now is that now, in the traning loop, we will iterate directly on the `tf.data.Dataset` generated by our `create_dataset` function. \n",
    "\n",
    "We will configure the dataset so that it iterates 250 times over our synthetic dataset in batches of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #2:** Complete the code in the cell below to call your dataset above when training the model. Note that the `step, (X_batch, Y_batch)` iterates over the `dataset`. The inside of the `for` loop should be exactly as in the previous lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0 - loss: 109.76800537109375, w0: 0.23999999463558197, w1: 0.4399999976158142\n",
      "\n",
      "STEP 100 - loss: 9.363959312438965, w0: 2.55655837059021, w1: 6.674341678619385\n",
      "\n",
      "STEP 200 - loss: 1.393267273902893, w0: 2.2146825790405273, w1: 8.717182159423828\n",
      "\n",
      "STEP 300 - loss: 0.20730558037757874, w0: 2.082810878753662, w1: 9.505172729492188\n",
      "\n",
      "STEP 400 - loss: 0.03084510937333107, w0: 2.03194260597229, w1: 9.809128761291504\n",
      "\n",
      "STEP 500 - loss: 0.004589457996189594, w0: 2.012321710586548, w1: 9.926374435424805\n",
      "\n",
      "STEP 600 - loss: 0.0006827632314525545, w0: 2.0047526359558105, w1: 9.971602439880371\n",
      "\n",
      "STEP 700 - loss: 0.00010164896957576275, w0: 2.0018346309661865, w1: 9.989042282104492\n",
      "\n",
      "STEP 800 - loss: 1.5142451957217418e-05, w0: 2.000706911087036, w1: 9.995771408081055\n",
      "\n",
      "STEP 900 - loss: 2.256260358990403e-06, w0: 2.0002737045288086, w1: 9.998367309570312\n",
      "\n",
      "STEP 1000 - loss: 3.3405058275093324e-07, w0: 2.000105381011963, w1: 9.999371528625488\n",
      "\n",
      "STEP 1100 - loss: 4.977664502803236e-08, w0: 2.000040054321289, w1: 9.999757766723633\n",
      "\n",
      "STEP 1200 - loss: 6.475602276623249e-09, w0: 2.0000154972076416, w1: 9.99991226196289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO 2\n",
    "EPOCHS = 250\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = .02\n",
    "\n",
    "MSG = \"STEP {step} - loss: {loss}, w0: {w0}, w1: {w1}\\n\"\n",
    "\n",
    "w0 = tf.Variable(0.0)\n",
    "w1 = tf.Variable(0.0)\n",
    "\n",
    "dataset = create_dataset(X, Y, epochs=EPOCHS, batch_size=BATCH_SIZE) # TODO -- Your code here.\n",
    "\n",
    "for step, (X_batch, Y_batch) in enumerate(dataset): # TODO -- Your code here.\n",
    "\n",
    "    dw0, dw1 = compute_gradients(X_batch, Y_batch, w0, w1) # TODO -- Your code here.\n",
    "    # TODO -- Your code here.\n",
    "    w0.assign_sub(dw0 * LEARNING_RATE)\n",
    "    w1.assign_sub(dw1 * LEARNING_RATE)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        loss = loss_mse(X_batch, Y_batch, w0, w1) # TODO -- Your code here.\n",
    "        print(MSG.format(step=step, loss=loss, w0=w0.numpy(), w1=w1.numpy()))\n",
    "        \n",
    "assert loss < 0.0001\n",
    "assert abs(w0 - 2) < 0.001\n",
    "assert abs(w1 - 10) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating the CSV files\n",
    "\n",
    "We will start with the **taxifare dataset** CSV files that we wrote out in a previous lab. \n",
    "\n",
    "The taxifare dataset files have been saved into `../toy_data`.\n",
    "\n",
    "Check that it is the case in the cell below, and, if not, regenerate the taxifare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#$my - works in Linux/Unix system, ie. G Colab\n",
    "!ls -l ../toy_data/taxi*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 chq-anyac 1049089  61473 Mar 23 13:16 data/toy_data/taxi-test.csv\n",
      "-rw-r--r-- 1 chq-anyac 1049089 288831 Mar 23 13:16 data/toy_data/taxi-train.csv\n",
      "-rw-r--r-- 1 chq-anyac 1049089  61082 Mar 23 13:16 data/toy_data/taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -l data/toy_data/taxi*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$my Preview a .csv file  \n",
    "Refer to my_Intro2TF_load_diff_filedata.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3,2012-01-03 19:21:35 UTC,-73.962627,40.763214,-73.973485,40.753353,1,0\n",
      "25.3,2010-09-27 07:30:15 UTC,-73.965799,40.794243,-73.927134,40.852261,3,1\n",
      "27.5,2015-05-19 00:40:02 UTC,-73.86344146728516,40.76899719238281,-73.96058654785156,40.76129913330078,1,2\n",
      "5.7,2010-04-29 12:28:00 UTC,-73.989255,40.738912,-73.97558,40.749172,1,3\n",
      "11.5,2013-06-23 06:08:09 UTC,-73.99731,40.763735,-73.955657,40.768141,1,4\n",
      "18.0,2014-10-14 18:52:03 UTC,-73.997995,40.761638,-74.008985,40.712442,1,5\n",
      "4.9,2010-04-29 12:28:00 UTC,-73.977315,40.766182,-73.970845,40.761462,5,6\n",
      "32.33,2014-02-24 18:22:00 UTC,-73.985358,40.761352,-73.92427,40.699145,1,7\n",
      "17.0,2015-03-26 02:48:58 UTC,-73.93981170654297,40.846473693847656,-73.97361755371094,40.786983489990234,1,8\n",
      "12.5,2013-04-09 09:39:13 UTC,-73.977323,40.753934,-74.00719,40.741472,1,9\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head 'data/toy_data/taxi-valid.csv' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.data to read the CSV files\n",
    "\n",
    "The `tf.data` API can easily read csv files using the helper function tf.data.experimental.make_csv_dataset\n",
    "\n",
    "If you have TFRecords (which is recommended), you may use tf.data.experimental.make_batched_features_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to define \n",
    "\n",
    "- the feature names into a list `CSV_COLUMNS`\n",
    "- their default values into a list `DEFAULTS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = [\n",
    "    'fare_amount',\n",
    "    'pickup_datetime',\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count',\n",
    "    'key'\n",
    "]\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now wrap the call to `make_csv_dataset` into its own function that will take only the file pattern (i.e. glob) where the dataset files are to be located:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #3:** Complete the code in the `create_dataset(...)` function below to return a `tf.data` dataset made from the `make_csv_dataset`. Have a look at the [documentation here](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset). The `pattern` will be given as an argument of the function but you should set the `batch_size`, `column_names` and `column_defaults`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3\n",
    "def create_dataset(pattern):\n",
    "    # TODO -- Your code here.\n",
    "    #$myComment - no label\n",
    "    dataset = tf.data.experimental.make_csv_dataset(pattern, 1, CSV_COLUMNS, DEFAULTS)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: OrderedDict([(fare_amount, (1,)), (pickup_datetime, (1,)), (pickup_longitude, (1,)), (pickup_latitude, (1,)), (dropoff_longitude, (1,)), (dropoff_latitude, (1,)), (passenger_count, (1,)), (key, (1,))]), types: OrderedDict([(fare_amount, tf.float32), (pickup_datetime, tf.string), (pickup_longitude, tf.float32), (pickup_latitude, tf.float32), (dropoff_longitude, tf.float32), (dropoff_latitude, tf.float32), (passenger_count, tf.float32), (key, tf.string)])>\n"
     ]
    }
   ],
   "source": [
    "tempds = create_dataset('data/toy_data/taxi-valid*') #$my class tensorflow.python.data.ops.dataset_ops.PrefetchDataset\n",
    "print(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $my Preview what's in tempds - by iteration over items in tempds\n",
    "Not recommended to iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fare_amount', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([10.9], dtype=float32)>), ('pickup_datetime', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'2010-12-21 13:08:00 UTC'], dtype=object)>), ('pickup_longitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.994896], dtype=float32)>), ('pickup_latitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.740196], dtype=float32)>), ('dropoff_longitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.96087], dtype=float32)>), ('dropoff_latitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.757412], dtype=float32)>), ('passenger_count', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>), ('key', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'169'], dtype=object)>)])\n",
      "\n",
      "OrderedDict([('fare_amount', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([30.83], dtype=float32)>), ('pickup_datetime', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'2014-12-08 21:50:00 UTC'], dtype=object)>), ('pickup_longitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.97326], dtype=float32)>), ('pickup_latitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.75811], dtype=float32)>), ('dropoff_longitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.87136], dtype=float32)>), ('dropoff_latitude', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.71925], dtype=float32)>), ('passenger_count', <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>), ('key', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'201'], dtype=object)>)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#$my preview what's in tempds - by iteration over items in tempds\n",
    "i = 0\n",
    "i_exit = 2\n",
    "\n",
    "for item in tempds: # in class tensorflow.python.data.ops.dataset_ops.PrefetchDataset\n",
    "    if i == i_exit: \n",
    "        break #exit here\n",
    "    \n",
    "    print(item) #class 'collections.OrderedDict'\n",
    "    print()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare_amount: [4.5]\n",
      "pickup_datetime: [b'2012-04-06 15:52:48 UTC']\n",
      "pickup_longitude: [-73.992455]\n",
      "pickup_latitude: [40.739414]\n",
      "dropoff_longitude: [-74.00201]\n",
      "dropoff_latitude: [40.74503]\n",
      "passenger_count: [1.]\n",
      "key: [b'415']\n",
      "\n",
      "fare_amount: [8.5]\n",
      "pickup_datetime: [b'2011-12-03 10:28:00 UTC']\n",
      "pickup_longitude: [-73.99092]\n",
      "pickup_latitude: [40.751083]\n",
      "dropoff_longitude: [-73.96866]\n",
      "dropoff_latitude: [40.75236]\n",
      "passenger_count: [1.]\n",
      "key: [b'91']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#$my preview what's in tempds - by iteration over items in tempds\n",
    "i = 0\n",
    "i_exit = 2\n",
    "\n",
    "for item in tempds: # in class tensorflow.python.data.ops.dataset_ops.PrefetchDataset\n",
    "    if i == i_exit: \n",
    "        break #exit here\n",
    "    \n",
    "    #print(item) #class 'collections.OrderedDict'\n",
    "    #refer to https://www.geeksforgeeks.org/ordereddict-in-python/\n",
    "    for k,v in item.items(): # in items of class collections.OrderedDict \n",
    "        print(\"{}: {}\".format(k,v))\n",
    "        \n",
    "    print()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to iterate over items in tempds (not recommended to iterate)  \n",
    "`next` method with an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fare_amount',\n",
       "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([11.7], dtype=float32)>),\n",
       "             ('pickup_datetime',\n",
       "              <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'2009-05-27 20:37:00 UTC'], dtype=object)>),\n",
       "             ('pickup_longitude',\n",
       "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.95558], dtype=float32)>),\n",
       "             ('pickup_latitude',\n",
       "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.768337], dtype=float32)>),\n",
       "             ('dropoff_longitude',\n",
       "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.98626], dtype=float32)>),\n",
       "             ('dropoff_latitude',\n",
       "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.73497], dtype=float32)>),\n",
       "             ('passenger_count',\n",
       "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)>),\n",
       "             ('key',\n",
       "              <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'211'], dtype=object)>)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#$my preview what's in tempds\n",
    "iterator = iter(tempds)\n",
    "example = next(iterator) #class collections.OrderedDict\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare_amount\n",
      "tf.Tensor([11.7], shape=(1,), dtype=float32)\n",
      "pickup_datetime\n",
      "tf.Tensor([b'2009-05-27 20:37:00 UTC'], shape=(1,), dtype=string)\n",
      "pickup_longitude\n",
      "tf.Tensor([-73.95558], shape=(1,), dtype=float32)\n",
      "pickup_latitude\n",
      "tf.Tensor([40.768337], shape=(1,), dtype=float32)\n",
      "dropoff_longitude\n",
      "tf.Tensor([-73.98626], shape=(1,), dtype=float32)\n",
      "dropoff_latitude\n",
      "tf.Tensor([40.73497], shape=(1,), dtype=float32)\n",
      "passenger_count\n",
      "tf.Tensor([5.], shape=(1,), dtype=float32)\n",
      "key\n",
      "tf.Tensor([b'211'], shape=(1,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#$my preview\n",
    "for k,v in example.items():\n",
    "    print(k)\n",
    "    print(v) #class 'tensorflow.python.framework.ops.EagerTensor'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $my Preview what's in tempds - without iteration\n",
    "Start by reusing custom function `show_batch` from load_diff_filedata.ipynb  \n",
    "`take()` method of `tf.data.Dataset` used for limiting number of items in dataset\n",
    "\n",
    "How to use dataset api take() method in TensorFlow https://www.gcptutorials.com/article/how-to-use-take-method-in-tensorflow  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$myComment: this code from load_diff_filedata.ipynb wouldn't work here because tempds doesn't contain a label\n",
    "def show_batch(dataset):\n",
    "  for batch, label in dataset.take(1):\n",
    "    print()\n",
    "    for key, value in batch.items():\n",
    "      print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "\n",
    "#show_batch(tempds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fare_amount         : [22.5]\n",
      "pickup_datetime     : [b'2013-12-06 14:55:00 UTC']\n",
      "pickup_longitude    : [-73.980965]\n",
      "pickup_latitude     : [40.733597]\n",
      "dropoff_longitude   : [-73.971825]\n",
      "dropoff_latitude    : [40.790337]\n",
      "passenger_count     : [1.]\n",
      "key                 : [b'471']\n",
      "\n",
      "fare_amount         : [8.9]\n",
      "pickup_datetime     : [b'2009-05-27 20:37:00 UTC']\n",
      "pickup_longitude    : [-73.9611]\n",
      "pickup_latitude     : [40.760666]\n",
      "dropoff_longitude   : [-73.97741]\n",
      "dropoff_latitude    : [40.758404]\n",
      "passenger_count     : [2.]\n",
      "key                 : [b'534']\n"
     ]
    }
   ],
   "source": [
    "#$my - show batch without a label\n",
    "def show_batch(dataset):\n",
    "  for batch in dataset.take(2):\n",
    "    print()\n",
    "    for key, value in batch.items():\n",
    "      print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "\n",
    "show_batch(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $my Cleaner preview offered by Solution \n",
    "Refer to 2_dataset_api_solution.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropoff_latitude': array([40.750004], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.991234], dtype=float32),\n",
      " 'fare_amount': array([6.5], dtype=float32),\n",
      " 'key': array([b'88'], dtype=object),\n",
      " 'passenger_count': array([3.], dtype=float32),\n",
      " 'pickup_datetime': array([b'2009-09-24 21:36:16 UTC'], dtype=object),\n",
      " 'pickup_latitude': array([40.75538], dtype=float32),\n",
      " 'pickup_longitude': array([-73.97921], dtype=float32)}\n",
      "\n",
      "\n",
      "{'dropoff_latitude': array([40.75586], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.9728], dtype=float32),\n",
      " 'fare_amount': array([26.1], dtype=float32),\n",
      " 'key': array([b'333'], dtype=object),\n",
      " 'passenger_count': array([1.], dtype=float32),\n",
      " 'pickup_datetime': array([b'2012-02-19 23:10:09 UTC'], dtype=object),\n",
      " 'pickup_latitude': array([40.774113], dtype=float32),\n",
      " 'pickup_longitude': array([-73.87464], dtype=float32)}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's iterate over the first two element of this dataset using `dataset.take(2)`.\n",
    "# Then convert them ordinary Python dictionary with numpy array as values for more readability:\n",
    "for data in tempds.take(2):\n",
    "    pprint({k: v.numpy() for k, v in data.items()})\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a prefetched dataset, where each element is an `OrderedDict` whose keys are the feature names and whose values are tensors of shape `(1,)` (i.e. vectors).\n",
    "\n",
    "Let's iterate over the two first element of this dataset using `dataset.take(2)` and let's convert them ordinary Python dictionary with numpy array as values for more readability:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we really need is a dictionary of features + a label. So, we have to do two things to the above dictionary:\n",
    "\n",
    "1. Remove the unwanted column \"key\"\n",
    "1. Keep the label separate from the features\n",
    "\n",
    "Let's first implement a function that takes as input a row (represented as an `OrderedDict` in our `tf.data.Dataset` as above) and then returns a tuple with two elements:\n",
    "\n",
    "* The first element being the same `OrderedDict` with the label dropped\n",
    "* The second element being the label itself (`fare_amount`)\n",
    "\n",
    "Note that we will need to also remove the `key` and `pickup_datetime` column, which we won't use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #4a:** Complete the code in the `features_and_labels(...)` function below. Your function should return a dictionary of features and a label. Keep in mind `row_data` is already a dictionary and you will need to remove the `pickup_datetime` and `key` from `row_data` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNWANTED_COLS = ['pickup_datetime', 'key']\n",
    "\n",
    "# TODO 4a\n",
    "def features_and_labels(row_data):\n",
    "    label = row_data.pop(LABEL_COLUMN) # TODO -- Your code here.\n",
    "    features = row_data # TODO -- Your code here.\n",
    "    \n",
    "    # TODO -- Your code here.\n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        features.pop(unwanted_col)\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over 2 examples from our `tempds` dataset and apply our `feature_and_labels`\n",
    "function to each of the examples to make sure it's working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('pickup_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.98089], dtype=float32)>),\n",
      "             ('pickup_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.741863], dtype=float32)>),\n",
      "             ('dropoff_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.98028], dtype=float32)>),\n",
      "             ('dropoff_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.73056], dtype=float32)>),\n",
      "             ('passenger_count',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)])\n",
      "tf.Tensor([9.], shape=(1,), dtype=float32) \n",
      "\n",
      "OrderedDict([('pickup_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.98661], dtype=float32)>),\n",
      "             ('pickup_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.762264], dtype=float32)>),\n",
      "             ('dropoff_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.88637], dtype=float32)>),\n",
      "             ('dropoff_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.843903], dtype=float32)>),\n",
      "             ('passenger_count',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)>)])\n",
      "tf.Tensor([30.5], shape=(1,), dtype=float32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row_data in tempds.take(2):\n",
    "    features, label = features_and_labels(row_data)\n",
    "    pprint(features)\n",
    "    print(label, \"\\n\")\n",
    "    assert UNWANTED_COLS[0] not in features.keys()\n",
    "    assert UNWANTED_COLS[1] not in features.keys()\n",
    "    assert label.shape == [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $my Transforming the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw row data: \n",
      "OrderedDict([('fare_amount',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([13.], dtype=float32)>),\n",
      "             ('pickup_datetime',\n",
      "              <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'2013-12-21 06:21:00 UTC'], dtype=object)>),\n",
      "             ('pickup_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.96892], dtype=float32)>),\n",
      "             ('pickup_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.758507], dtype=float32)>),\n",
      "             ('dropoff_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.94245], dtype=float32)>),\n",
      "             ('dropoff_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.80632], dtype=float32)>),\n",
      "             ('passenger_count',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>),\n",
      "             ('key',\n",
      "              <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'778'], dtype=object)>)])\n",
      "Transformed row into target and features: \n",
      "tf.Tensor([13.], shape=(1,), dtype=float32)\n",
      "OrderedDict([('pickup_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.96892], dtype=float32)>),\n",
      "             ('pickup_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.758507], dtype=float32)>),\n",
      "             ('dropoff_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.94245], dtype=float32)>),\n",
      "             ('dropoff_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.80632], dtype=float32)>),\n",
      "             ('passenger_count',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>)])\n",
      "\n",
      "Raw row data: \n",
      "OrderedDict([('fare_amount',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.3], dtype=float32)>),\n",
      "             ('pickup_datetime',\n",
      "              <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'2011-10-05 08:14:00 UTC'], dtype=object)>),\n",
      "             ('pickup_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.98264], dtype=float32)>),\n",
      "             ('pickup_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.76715], dtype=float32)>),\n",
      "             ('dropoff_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.97174], dtype=float32)>),\n",
      "             ('dropoff_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.76375], dtype=float32)>),\n",
      "             ('passenger_count',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>),\n",
      "             ('key',\n",
      "              <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'298'], dtype=object)>)])\n",
      "Transformed row into target and features: \n",
      "tf.Tensor([5.3], shape=(1,), dtype=float32)\n",
      "OrderedDict([('pickup_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.98264], dtype=float32)>),\n",
      "             ('pickup_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.76715], dtype=float32)>),\n",
      "             ('dropoff_longitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-73.97174], dtype=float32)>),\n",
      "             ('dropoff_latitude',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([40.76375], dtype=float32)>),\n",
      "             ('passenger_count',\n",
      "              <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)])\n"
     ]
    }
   ],
   "source": [
    "#$my Transforming the features manually\n",
    "#reusing LABEL_COLUMN and UNWANTED_COLS\n",
    "\n",
    "for row_data in tempds.take(2): #class 'collections.OrderedDict'\n",
    "    \n",
    "    print(\"\\nRaw row data: \")\n",
    "    pprint(row_data)\n",
    "    \n",
    "    #transform into features and target\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    print(\"Transformed row into target and features: \")\n",
    "    print(label)\n",
    "    [row_data.pop(unwanted_col) for unwanted_col in UNWANTED_COLS]     \n",
    "    pprint(row_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now refactor our `create_dataset` function so that it takes an additional argument `batch_size` and batch the data correspondingly. We will also use the `features_and_labels` function we implemented for our dataset to produce tuples of features and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #4b:** Complete the code in the `create_dataset(...)` function below to return a `tf.data` dataset made from the `make_csv_dataset`. Now, the `pattern` and `batch_size` will be given as an arguments of the function but you should set the `column_names` and `column_defaults` as before. You will also apply a `.map(...)` method to create features and labels from each example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4b\n",
    "def create_dataset(pattern, batch_size):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "\n",
    "    dataset = dataset.map(features_and_labels) # TODO -- Your code here. #$myComment: fancy create_dataset()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that our batches are of the right size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropoff_latitude': array([40.762424, 40.728287], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.96815 , -73.995415], dtype=float32),\n",
      " 'passenger_count': array([1., 1.], dtype=float32),\n",
      " 'pickup_latitude': array([40.757572, 40.707973], dtype=float32),\n",
      " 'pickup_longitude': array([-73.9606  , -74.006546], dtype=float32)}\n",
      "[6.5 8.9] \n",
      "\n",
      "{'dropoff_latitude': array([40.768185, 40.722935], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.96859 , -74.004845], dtype=float32),\n",
      " 'passenger_count': array([1., 1.], dtype=float32),\n",
      " 'pickup_latitude': array([40.704605, 40.738243], dtype=float32),\n",
      " 'pickup_longitude': array([-74.00777 , -74.000015], dtype=float32)}\n",
      "[20.1  5.5] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "tempds = create_dataset('data/toy_data/taxi-train*', batch_size=2) #$my\n",
    "\n",
    "for X_batch, Y_batch in tempds.take(2):\n",
    "    pprint({k: v.numpy() for k, v in X_batch.items()})\n",
    "    print(Y_batch.numpy(), \"\\n\")\n",
    "    assert len(Y_batch) == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling\n",
    "\n",
    "When training a deep learning model in batches over multiple workers, it is helpful if we shuffle the data. That way, different workers will be working on different parts of the input file at the same time, and so averaging gradients across workers will help. Also, during training, we will need to read the data indefinitely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refactor our `create_dataset` function so that it shuffles the data, when the dataset is used for training.\n",
    "\n",
    "We will introduce an additional argument `mode` to our function to allow the function body to distinguish the case\n",
    "when it needs to shuffle the data (`mode == 'train'`) from when it shouldn't (`mode == 'eval'`).\n",
    "\n",
    "Also, before returning we will want to prefetch 1 data point ahead of time (`dataset.prefetch(1)`) to speed-up training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #4c:** The last step of our `tf.data` dataset will specify shuffling and repeating of our dataset pipeline. Complete the code below to add these three steps to the Dataset pipeline\n",
    "1. follow the `.map(...)` operation which extracts features and labels with a call to `.cache()` the result.\n",
    "2. during training, use `.shuffle(...)` and `.repeat()` to shuffle batches and repeat the dataset\n",
    "3. use `.prefetch(...)` to take advantage of multi-threading and speedup training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4c\n",
    "#$myComment: fancy input pipeline create_dataset()\n",
    "\n",
    "def create_dataset(pattern, batch_size=1, mode=\"eval\"):\n",
    "# The tf.data.experimental.make_csv_dataset() method reads CSV files into a dataset\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS) # TODO -- Your code here.\n",
    "\n",
    "# The map() function executes a specified function for each item in an iterable.\n",
    "# The item is sent to the function as a parameter.\n",
    "    dataset = dataset.map(features_and_labels).cache() # TODO -- Your code here.\n",
    "\n",
    "    if mode == \"train\":\n",
    "        dataset = dataset.shuffle(1000).repeat() # TODO -- Your code here.\n",
    "\n",
    "    # take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that our function works well in both modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(OrderedDict([('pickup_longitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-73.95741, -73.99501], dtype=float32)>), ('pickup_latitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([40.76948 , 40.744877], dtype=float32)>), ('dropoff_longitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-73.98585, -73.99646], dtype=float32)>), ('dropoff_latitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([40.76165, 40.73549], dtype=float32)>), ('passenger_count', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>)]), <tf.Tensor: shape=(2,), dtype=float32, numpy=array([8.6, 5. ], dtype=float32)>)]\n"
     ]
    }
   ],
   "source": [
    "tempds = create_dataset('data/toy_data/taxi-train*', 2, 'train')\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(OrderedDict([('pickup_longitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-73.91153, -73.98536], dtype=float32)>), ('pickup_latitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([40.76809 , 40.761353], dtype=float32)>), ('dropoff_longitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-73.939026, -73.92427 ], dtype=float32)>), ('dropoff_latitude', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([40.81655 , 40.699146], dtype=float32)>), ('passenger_count', <tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 1.], dtype=float32)>)]), <tf.Tensor: shape=(2,), dtype=float32, numpy=array([20.83, 32.33], dtype=float32)>)]\n"
     ]
    }
   ],
   "source": [
    "tempds = create_dataset('data/toy_data/taxi-valid*', 2, 'eval')\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we will build the model using this input pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m91"
  },
  "kernelspec": {
   "display_name": "anya_tf2",
   "language": "python",
   "name": "anya_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
